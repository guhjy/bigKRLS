% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/bigKRLS.R
\name{bigKRLS}
\alias{bigKRLS}
\title{bigKRLS: Runtime and Memory Optimized Kernel Regularized Least Squares}
\usage{
bigKRLS(y = NULL, X = NULL, sigma = NULL, derivative = TRUE,
  which.derivatives = NULL, vcov.est = TRUE, lambda = NULL, L = NULL,
  U = NULL, tol = NULL, noisy = TRUE, model_subfolder_name = NULL,
  overwrite.existing = F, Ncores = NULL)
}
\arguments{
\item{y}{A vector of observations on the dependent variable; missing values not allowed. May be base R matrix or library(bigmemory) big.matrix.}

\item{X}{A matrix of observations of the independent variables; factors, missing values, and constant vectors not allowed. May be base R matrix or library(bigmemory) big.matrix.}

\item{sigma}{Bandwidth parameter, shorthand for sigma squared. Default: sigma <- ncol(X). Since x variables are standardized, facilitates interprepation of the Gaussian kernel, exp(-dist(X)^2/sigma) a.k.a the similarity score. Of course, if dist between observation i and j is 0, there similarity is 1 since exp(0) = 1. Suppose i and j differ by one standard deviation on each dimension. Then the similarity is exp(-ncol(X)/sigma) = exp(-1) = 0.368.}

\item{derivative}{Logical: Estimate derivatives (as opposed to just coefficients)? Recommended for interpretability.}

\item{which.derivatives}{Optional. For which columns of X should marginal effects be estimated ("variables of interest"). If derivative=TRUE and which.derivative=NULL, all will marginal effects estimated (default settings). Example: out = bigKRLS(..., which.derivatives = c(1, 3, 5))}

\item{vcov.est}{Logical: Estimate variance covariance matrix? Required to obtain derivatives and standard errors on predictions (default = TRUE).}

\item{lambda}{Regularization parameter. Default: estimated based (in part) on the eigenvalues of the kernel via Golden Search with convergence parameter "tolerance." Must be positive, real number.}

\item{L}{Lower bound of Golden Search for lambda.}

\item{U}{Upper bound of Golden Search for lambda.}

\item{tol}{tolerance parameter for Golden Search for lambda. Default: N / 1000.}

\item{noisy}{Logical: Display progress to console (intermediate output, time stamps, etc.)? (Recommended particularly for SSH users, who should also use X11 forwarding to see Rcpp progress display.)}

\item{model_subfolder_name}{If not null, will save estimates to this subfolder of your current working directory. Alternatively, use save.bigKRLS() on the outputted object.}

\item{overwrite.existing}{Logical: overwrite contents in folder 'model_subfolder_name'? If FALSE, appends lowest possible number to model_subfolder_name name (e.g., ../myresults3/).}

\item{Ncores}{Number of processor cores to use. Default = ncol(X) or N - 2 (whichever is smaller). More than N - 2 NOT recommended. Uses library(snow) unless Ncores = 1.}
}
\value{
bigKRLS Object containing slope and uncertainty estimates; summary() and predict() defined for class bigKRLS, as is shiny.bigKRLS().
}
\description{
bigKRLS: Runtime and Memory Optimized Kernel Regularized Least Squares
}
\examples{
N <- 500  # proceed with caution above N = 5,000 for system with 8 gigs made avaiable to R
P <- 4
X <- matrix(rnorm(N*P), ncol=P)
X <- cbind(X, sample(0:1, replace = TRUE, size = nrow(X)))
b <- runif(ncol(X))
y <- X \%*\% b + rnorm(nrow(X))
out <- bigKRLS(y, X)
}

